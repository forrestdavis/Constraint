{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9abc040-2afd-41bc-9bf2-75a1a84991a8",
   "metadata": {},
   "source": [
    "# Lab 5: RNNs for sequence prediction\n",
    "#### Prof. Forrest Davis, Colgate University\n",
    "\n",
    "The goal of this notebook is to introduce you to PyTorch and give you hands-on experience with building and using RNNs.\n",
    "\n",
    "Here are some learning objectives for this lab:\n",
    "\n",
    "1. Build an intution for how RNNs work\n",
    "2. Describe and apply some key aspects of PyTorch\n",
    "3. Understand how to create a training pipeline for a neural network\n",
    "4. Describe the merits of different evaluation approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f604e189-c5de-4fcf-ab07-8cfa6c2af19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "import time "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3dc85c2-b79c-4323-88d8-fdb5475ab5eb",
   "metadata": {},
   "source": [
    "## Part 1: Load and preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a2051c-e435-4384-9d7c-5c390a67d7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('addition_data.csv')\n",
    "train = data[data['split'] == 'train']\n",
    "valid = data[data['split'] == 'valid']\n",
    "test = data[data['split'] == 'test']\n",
    "print(train.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f71230-4423-447e-a4d0-a677e615d492",
   "metadata": {},
   "source": [
    "[**Written Answer**] What is our task (e.g., what is our output showing)? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05d1ab3-99da-44d2-b812-8203fe8ef0c5",
   "metadata": {},
   "source": [
    "[**Code Answer**] Complete `encode_data` and `decode_data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3842fbee-42a5-47fa-a62a-60c4886904c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab() -> dict:\n",
    "    \"\"\" Get the relevant vocab \"\"\"\n",
    "    mapping = {}\n",
    "    for i in range(10):\n",
    "        mapping[str(i)] = i\n",
    "\n",
    "    # Add special chars\n",
    "    mapping[' '] = 10\n",
    "    mapping['+'] = 11\n",
    "    mapping['='] = 12\n",
    "    mapping['?'] = 13\n",
    "    \n",
    "    return mapping \n",
    "    \n",
    "def encode_data(data: np.array, mapping: dict) -> np.array: \n",
    "    \"\"\" Format our input into samples of tens ones op tens ones. \n",
    "    Args:\n",
    "        data (np.array): Array of input strings\n",
    "        mapping (dict): A mapping from characters to ids (i.e. numbers)\n",
    "\n",
    "    Returns: \n",
    "        (np.array): Encoded samples \n",
    "\n",
    "    For example, with data as [\"76 + 26 = ???\"] and mapping the output of get_vocab,\n",
    "    then the output should be [ 7  6 10 11 10  2  6 10 12 10 13 13 13], \n",
    "    Notice that there are numbers for each character (including the space character).\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "def decode_data(data: np.array, mapping: dict) -> list:\n",
    "    \"\"\" Converted encoded data back into strings\n",
    "    Args:\n",
    "        data (np.array): Array of encoded data (i.e. numbers)\n",
    "        mapping (dict): A mapping from characters to ids (i.e. numbers)\n",
    "\n",
    "    Returns: \n",
    "        (np.array): Decoded samples \n",
    "\n",
    "    For example, with data as [ 7  6 10 11 10  2  6 10 12 10 13 13 13] and mapping the output of get_vocab,\n",
    "    then the output should be [\"76 + 26 = ???\"]\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27cd696-58cd-4816-b49b-5b77ffe3b23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = get_vocab()\n",
    "X = encode_data(train['input'].to_numpy(), mapping)\n",
    "print(X)\n",
    "Y = encode_data(train['output'].to_numpy(), mapping)\n",
    "\n",
    "# Helpful print statements\n",
    "#print(train['input'].to_numpy()[0], X[0], decode_data(X, mapping)[0])\n",
    "#print(train['output'].to_numpy()[0], Y[0], decode_data(Y, mapping)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad90a65b-14ac-4784-b887-b92013a029d7",
   "metadata": {},
   "source": [
    "Consider the following code snippet. \n",
    "\n",
    "[**Written Answer**] What does `one_hot` do and what does the line containing `labels_again` do and how? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bdde077-d500-4d51-beff-8f2b7d62964a",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = torch.randint(0, 10, (10,))\n",
    "one_hot = torch.nn.functional.one_hot(labels)\n",
    "labels_again = torch.argmax(one_hot, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e041515-3cb4-4f70-bcfc-2be791ec1e44",
   "metadata": {},
   "source": [
    "[**Writen Answer**] Notice that we are using torch [tensors](https://pytorch.org/docs/stable/tensors.html). What are they?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856c130d-e4a2-45cf-800e-4078ac4c5925",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make tensors and one hot\n",
    "def oneHot(data: np.array, mapping: dict) -> torch.tensor:\n",
    "    d = torch.tensor(data).long()\n",
    "    return torch.nn.functional.one_hot(d, len(mapping)).float()\n",
    "\n",
    "def unHot(data: torch.tensor, mapping: dict) -> np.array:\n",
    "    return torch.argmax(data, dim=-1).numpy()\n",
    "\n",
    "assert (X == unHot(oneHot(X, mapping), mapping)).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda78f6c-b7e8-488c-9246-d62557bd0ed8",
   "metadata": {},
   "source": [
    "## Part 2: Investigate a RNN\n",
    "\n",
    "Below is a scaffold for a minimal implementation of a RNN that solves our addition task. Your task in this section is to complete the code. Consider PyTorch's [RNN](https://pytorch.org/docs/stable/generated/torch.nn.RNN.html) and other [documentation](https://pytorch.org/docs/stable/index.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cae485b-dcd5-43ba-86b1-23641b1312ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, nInput, nHidden, nLayers, batchFirst = True):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.nInput = nInput\n",
    "        self.nHidden = nHidden\n",
    "        self.nLayers = nLayers\n",
    "        self.batchFirst = batchFirst\n",
    "        # Need to set an RNN and a decoder which maps from the last \n",
    "        # layer of the RNN to the predictions\n",
    "\n",
    "    def forward(self, observation, hidden):\n",
    "        \"\"\" Does the forward computation \n",
    "\n",
    "        Parameters: \n",
    "            observation (torch.Tensor): Tokenized input to the model \n",
    "            hidden (torch.Tensor): Prior hidden representations\n",
    "        Returns: \n",
    "            torch.Tensor: logits of the model      \n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def init_hidden(self, batchSize):\n",
    "        \"\"\" Returns an empty hidden representation of size number of layers X batch size X number \n",
    "            of hidden units\n",
    "\n",
    "        Parameters:\n",
    "            batchSize (int): Batch size\n",
    "        Returns:\n",
    "            torch.tensor: All zero's hidden representation to use for the first input in a sequence\n",
    "        \"\"\"\n",
    "        return torch.zeros((self.nLayers, batchSize, self.nHidden), dtype=torch.float)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39fed039-68c7-4d28-8284-7a2c282caf0b",
   "metadata": {},
   "source": [
    "Imagine we have two samples, each with a sequence length of 6 and an input dimensionality of 4. The following code snippet runs a forward pass on such a case with dummy data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a354f412-7e3e-4dcc-88d0-7fde00fb57bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.rand((2, 6, 4))\n",
    "print(X)\n",
    "print(X.shape)\n",
    "\n",
    "m = RNNModel(nInput=4, nHidden=5, nLayers=2, batch_first=True)\n",
    "hidden = m.init_hidden(2)\n",
    "output, hidden = m.forward(X, hidden)\n",
    "print(output)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5804fc7-5d4b-4c51-a0a5-11d08a59ec80",
   "metadata": {},
   "source": [
    "## Part 3: Train and Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d9e7ce-f96e-4329-84d8-b8a6c090e48d",
   "metadata": {},
   "source": [
    "[**Written Answer**] Answer the following questions\n",
    "\n",
    "1. What types of objects are the model, loss, and cost?\n",
    "2. Why do we have `@torch.no_grad()` in front of accuracy and eval loss? What does this tell us about the model's default behavior? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c78531-8b08-4323-b18f-54d479125c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(X, Y, model, nEpochs, batchSize, lr):\n",
    "    model.train()\n",
    "    \n",
    "    cost = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9, nesterov=True)\n",
    "    for epoch in range(nEpochs):\n",
    "        total_loss = 0.\n",
    "        for idx in range(0, X.size(0), batchSize):\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            batch_X = X[idx:idx+batchSize, :, :]\n",
    "            batch_Y = Y[idx:idx+batchSize,:]\n",
    "\n",
    "            if batch_X.size(0) != batchSize:\n",
    "                continue\n",
    "            \n",
    "            hidden = model.init_hidden(batchSize)\n",
    "            logits, hidden = model.forward(batch_X, hidden)\n",
    "            loss = cost(logits.reshape(-1, model.nInput), batch_Y.flatten().long())\n",
    "            total_loss += loss.item()\n",
    "            loss.backward()\n",
    "            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 0.25)\n",
    "            optimizer.step()\n",
    "        print(f\"Epoch {epoch}, Total Loss: {total_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc07769-76ea-454a-8060-8f4374be4ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up data\n",
    "mapping = get_vocab()\n",
    "X = encode_data(train['input'].to_numpy(), mapping)\n",
    "print(X)\n",
    "Y = encode_data(train['output'].to_numpy(), mapping)\n",
    "encoded_X = oneHot(X, mapping)\n",
    "encoded_Y = torch.tensor(Y)[:,-3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac6b1e5-95cb-4bb6-9daa-68c0384a402d",
   "metadata": {},
   "outputs": [],
   "source": [
    "nInput = encoded_X.shape[-1]\n",
    "nHidden = 50\n",
    "nLayers = 2\n",
    "model = RNNModel(nInput, nHidden, nLayers)\n",
    "train_model(encoded_X, encoded_Y, model, 15, 20, 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f1771d-586f-4573-830d-b374f42966e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evalLoss(X, Y, model):\n",
    "    model.eval()\n",
    "    total_loss = 0.\n",
    "    cost = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    for idx in range(X.size(0)):\n",
    "        hidden = model.init_hidden(1)\n",
    "        logits, hidden = model.forward(X[idx,:,:].unsqueeze(0), hidden)\n",
    "        loss = cost(logits.reshape(-1, model.nInput), Y[idx,:].flatten().long())\n",
    "        total_loss += loss.item()\n",
    "    return total_loss/X.size(0)\n",
    "\n",
    "def getPredictions(logits):\n",
    "    probabilities = torch.nn.functional.softmax(logits, dim=-1)\n",
    "    return torch.argmax(probabilities, -1)\n",
    "\n",
    "@torch.no_grad()\n",
    "def accuracy(X, Y, model):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for idx in range(X.size(0)):\n",
    "        hidden = model.init_hidden(1)\n",
    "        logits, hidden = model.forward(X[idx,:,:].unsqueeze(0), hidden) \n",
    "        yhat = getPredictions(logits).flatten()\n",
    "        ytrue = Y[idx, :].flatten()\n",
    "        if (yhat == ytrue).all():\n",
    "            #print(decode_data(unHot(X[idx, :, :].unsqueeze(0), mapping), mapping), ytrue)\n",
    "            correct += 1\n",
    "        total += 1\n",
    "        \n",
    "    return round(100*correct/total, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e41efee-39d8-415f-a854-15b5e0a43a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_X = oneHot(encode_data(valid['input'].to_numpy(), mapping), mapping)\n",
    "valid_Y = torch.tensor(encode_data(valid['output'].to_numpy(), mapping))[:, -3:]\n",
    "accuracy(valid_X, valid_Y, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9867f3-cec7-433b-9433-d80a19c14338",
   "metadata": {},
   "source": [
    "[**Code Answer**] Find better hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b30b9f-703e-4a87-b779-d68ec682c3e9",
   "metadata": {},
   "source": [
    "The following functions permit you to interact with your model by putting in expressions and seeing the model's predictions. \n",
    "\n",
    "[**Code Answer**] Play around with your model and try to find some cases that it gets wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6e6c54-8d61-498a-a7b0-c9758c86a020",
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkExpression(expression: str) -> bool:\n",
    "    parts = expression.split(' ')\n",
    "    if parts[0] == 'n':\n",
    "        return True\n",
    "    if len(parts) != 3:\n",
    "        print(\"Expression is invalid. Valid expression are like 01 + 03\")\n",
    "        return False\n",
    "    # Two digit numbers on left hand side\n",
    "    if (len(parts[0]) != 2 or len(parts[2]) != 2):\n",
    "        print('Left hand side has two two digit numbers')\n",
    "        return False\n",
    "    if parts[1] != '+':\n",
    "        print('Need the +')\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "@torch.no_grad()\n",
    "def interact(model, mapping):\n",
    "    model.eval()\n",
    "    while True:\n",
    "        getInput = True\n",
    "        while getInput:\n",
    "            expression = input(\"Enter an expression (n to stop): \")\n",
    "            if checkExpression(expression):\n",
    "                getInput=False\n",
    "        if expression == 'n':\n",
    "            break\n",
    "        X = oneHot(encode_data(np.array([expression+' = ???']), mapping), mapping)\n",
    "        hidden = model.init_hidden(1)\n",
    "        logits, _ = model(X, hidden)\n",
    "        prediction = getPredictions(logits).flatten()\n",
    "        output = ''.join(map(lambda x: str(int(x)), prediction))\n",
    "        print(f\"The model predicts: {output}\")\n",
    "\n",
    "interact(model, mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e319e56-29dc-4574-b9ec-1a2689313f4e",
   "metadata": {},
   "source": [
    "## Part 4: Fun different data splits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e556ae33-ed0b-49dc-8cd4-53769e86c16a",
   "metadata": {},
   "source": [
    "Consider the following data and training set up. \n",
    "\n",
    "[**Written Answer**] Explain why model performance is so much worse in this set up. Come up with a hypothesis for at least one other data split that might result in worse performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f7c2e9-5b51-49e1-9913-209ae73f8a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try one with no symmetry, so you only see one of x + y / y + x\n",
    "\n",
    "first_pair = {}\n",
    "second_pair = {}\n",
    "for i in range(0, 100):\n",
    "    for j in range(0, 100):\n",
    "        result = str(i + j) \n",
    "        str_i = str(i)\n",
    "        str_j = str(j)\n",
    "        if len(str_i) < 2:\n",
    "            str_i = '0'+str_i\n",
    "        if len(str_j) < 2:\n",
    "            str_j = '0' + str_j\n",
    "        result = '0'*(3-len(result))+result\n",
    "        v1 = f\"{str_i} + {str_j} = ???\"\n",
    "        v2 = f\"{str_j} + {str_i} = ???\"\n",
    "        if not(v1 in first_pair or v2 in first_pair):\n",
    "            first_pair[v1] = f\"{str_i} + {str_j} = {result}\"\n",
    "        else:\n",
    "            second_pair[v1] = f\"{str_i} + {str_j} = {result}\"\n",
    "\n",
    "for pair in first_pair:\n",
    "    assert pair not in second_pair\n",
    "\n",
    "# Put data in pandas dataframe so I can shuffle\n",
    "train_df = pd.DataFrame.from_dict({'input': list(first_pair.keys()), \n",
    "                                'output': list(first_pair.values())}).sample(frac=1).reset_index(drop=True)\n",
    "test_df = pd.DataFrame.from_dict({'input': list(second_pair.keys()), \n",
    "                                'output': list(second_pair.values())}).sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# Encode data \n",
    "mapping = get_vocab()\n",
    "X_train = oneHot(encode_data(train_df['input'].to_numpy(), mapping), mapping)\n",
    "Y_train = torch.tensor(encode_data(train_df['output'].to_numpy(), mapping))[:,-3:]\n",
    "X_test = oneHot(encode_data(test_df['input'].to_numpy(), mapping), mapping)\n",
    "Y_test = torch.tensor(encode_data(test_df['output'].to_numpy(), mapping))[:,-3:]\n",
    "\n",
    "# Train Model\n",
    "\n",
    "nInput = X_train.shape[-1]\n",
    "nHidden = 250\n",
    "nLayers = 3\n",
    "model = RNNModel(nInput, nHidden, nLayers)\n",
    "train_model(X_train, Y_train, model, 15, 20, 0.05)\n",
    "print('Accuracy', accuracy(X_test, Y_test, model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37b767c-a2a5-40aa-85f3-1d4159b6512a",
   "metadata": {},
   "source": [
    "[**Code Answer**] Time permitting, test your hypothesis!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
